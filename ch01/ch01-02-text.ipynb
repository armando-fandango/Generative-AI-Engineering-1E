{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac86e7e-e478-4b1f-9f15-dc8a9fe38d75",
   "metadata": {},
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a39dfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T05:13:21.954559Z",
     "start_time": "2023-05-04T05:13:21.938206Z"
    },
    "execution": {
     "iopub.execute_input": "2023-05-26T23:12:35.680112Z",
     "iopub.status.busy": "2023-05-26T23:12:35.679467Z",
     "iopub.status.idle": "2023-05-26T23:12:35.712406Z",
     "shell.execute_reply": "2023-05-26T23:12:35.711511Z",
     "shell.execute_reply.started": "2023-05-26T23:12:35.680049Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0] from path /opt/py/bin/python\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(f'Python {sys.version} from path {sys.executable}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c770df5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T05:16:15.480844Z",
     "start_time": "2023-05-04T05:16:15.451147Z"
    },
    "execution": {
     "iopub.execute_input": "2023-05-26T23:12:35.715698Z",
     "iopub.status.busy": "2023-05-26T23:12:35.715361Z",
     "iopub.status.idle": "2023-05-26T23:12:37.783800Z",
     "shell.execute_reply": "2023-05-26T23:12:37.782761Z",
     "shell.execute_reply.started": "2023-05-26T23:12:35.715667Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers 4.29.2\n",
      "torch 2.0.1 on cuda:0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(f'transformers {transformers.__version__}')\n",
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'torch {torch.__version__} on {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8dc20f",
   "metadata": {},
   "source": [
    "## generate text from huggingface hub models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5072f1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T05:20:30.538290Z",
     "start_time": "2023-05-04T05:20:25.396894Z"
    },
    "execution": {
     "iopub.execute_input": "2023-05-26T23:12:37.785753Z",
     "iopub.status.busy": "2023-05-26T23:12:37.784971Z",
     "iopub.status.idle": "2023-05-26T23:12:37.830825Z",
     "shell.execute_reply": "2023-05-26T23:12:37.830009Z",
     "shell.execute_reply.started": "2023-05-26T23:12:37.785717Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformers.set_seed(42)\n",
    "prompt = \"What is a Transformer?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b9187a-2dc0-43ed-ae48-622d9f1282f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T23:12:37.832362Z",
     "iopub.status.busy": "2023-05-26T23:12:37.831946Z",
     "iopub.status.idle": "2023-05-26T23:12:48.799421Z",
     "shell.execute_reply": "2023-05-26T23:12:48.798437Z",
     "shell.execute_reply.started": "2023-05-26T23:12:37.832324Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a Transformer?\n",
      "\n",
      "There aren't lots of really accurate definitions of a transformer, but there are some basic things like a transformer (a type of system which allows for certain modes of operation under different voltages) and an amplifier (a system which allows for specific output characteristics). To get an idea of how different modes of operation work, consider a transistor or transistor at 20立. While in many fields this is less than ideal in every job where a voltage of 0 - 40立 is required, there is a tradeoff. In general, such a transistor will generate a more potent and sensitive signal at a lower power level. However, in a job where the output voltage of the transmeasurement switch is less than 5% of the actual output voltage, the transmeasurement switch is more sensitive and thus more capable of processing voltages higher than the required voltage.\n",
      "\n",
      "In order to find the optimal level of sensitivity for transistors and transistors at a given power level, we need to calculate the capacitance per unit mass of the voltage in addition to the current (see below for details). This capacitance represents a number determined simply by the amount of current in a current, at constant power-to-volts (which often means the voltage is in a high-voltage state) that the transistor needs for its input current (i.e., its voltage-to-current ratio). In a nutshell, this means that given the exact same input current that we're considering in voltage-to-current ratios above 0 and above 4立 and equal to the capacitance of a certain unit mass of the voltage, the current-to-volts per unit mass of a certain transistor will be constant. These values provide the level of sensitivity necessary to generate the maximum voltage differential.\n",
      "\n",
      "How Much Control Is It Needed?\n",
      "\n",
      "In the general terms of the transistor range of input voltage, we're looking at 4立 (4% power to\n"
     ]
    }
   ],
   "source": [
    "model = 'gpt2'\n",
    "pipe = transformers.pipeline('text-generation',model=model,device=device)\n",
    "response = pipe(prompt, max_new_tokens=400, num_return_sequences=1)\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f3d0e8-70d1-406e-93af-4c7deaf19336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T05:20:30.538290Z",
     "start_time": "2023-05-04T05:20:25.396894Z"
    },
    "execution": {
     "iopub.execute_input": "2023-05-26T23:12:48.802428Z",
     "iopub.status.busy": "2023-05-26T23:12:48.802013Z",
     "iopub.status.idle": "2023-05-26T23:12:53.947849Z",
     "shell.execute_reply": "2023-05-26T23:12:53.946532Z",
     "shell.execute_reply.started": "2023-05-26T23:12:48.802397Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a Transformer? This is about a single type of transformer (transformer).\n",
      "\n",
      "\n",
      "\n",
      "As for what I would like to discuss in future posts:\n",
      "I've got two questions that will be covered shortly.\n",
      "One is whether or not I will be in the process of adding a second type of super-simple transformer (if any). As for why I am doing it differently, perhaps, it means that the reason why is that my design team likes to be able to have a super-simple transformer that uses the same type of super-simple Super-type super-type, and the reason why this is the most important to me.\n",
      "But I want to stress, first, how much I want to make this super-simple (I hope to talk about some of that in a future post). In fact what, if anything, is my intent?\n",
      "First, I want to make this super-simple (I hope to talk about some of that in a future post). This is because it enables me to create a super-simple subtype of this type (if any). Why do I do it all, while keeping the same type of super-type super-type name, without leaving the same (even if that name is of the same type)?\n",
      "So, I'm trying to keep it simple, while keeping the same kind of super-type super-type name, without leaving the same (even if that name is of the same type)?\n",
      "So, I'm trying to create a super-simple subtype of this type, instead of being forced to put that super-type name away or use that type's super-type that I'm trying to define in one place, by that means use any of the super-typical super-type names above.\n",
      "Then, I'm trying to make that super-type super-type super-type super-type super-type type super-type super-type super-type super-type super-\n"
     ]
    }
   ],
   "source": [
    "model = 'distilgpt2'\n",
    "pipe = transformers.pipeline('text-generation',model=model,device=device)\n",
    "response = pipe(prompt, max_new_tokens=400, num_return_sequences=1)\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46ff5f97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T05:21:57.204280Z",
     "start_time": "2023-05-04T05:20:53.416586Z"
    },
    "execution": {
     "iopub.execute_input": "2023-05-26T23:12:53.950059Z",
     "iopub.status.busy": "2023-05-26T23:12:53.949587Z",
     "iopub.status.idle": "2023-05-26T23:13:21.560282Z",
     "shell.execute_reply": "2023-05-26T23:13:21.558972Z",
     "shell.execute_reply.started": "2023-05-26T23:12:53.950010Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a Transformer?\n",
      "\n",
      "A Transformer is a computer that is built around the Raspberry Pi and runs a program called Raspbian Jessie. This is a very user-friendly Linux distribution and runs inside a computer where you can connect to the Internet or access your camera, your phone, or even your TV, all with ease.\n",
      "\n",
      "How can I get my Raspberry Pi into my Transformer?\n",
      "\n",
      "The simplest way is to connect an HDMI to DVI to USB cable to your Transformer. You also need to get power for your device and connect it to the network using the Raspberry Pi GPIO (GPIO 16). Alternatively you can attach an AC USB power supply to your Transformer using the micro USB cable provided.\n"
     ]
    }
   ],
   "source": [
    "model = 'gpt2-large'\n",
    "pipe = transformers.pipeline('text-generation',model=model,device=device)\n",
    "response = pipe(prompt, max_new_tokens=400, num_return_sequences=1)\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9b93a-b51f-4bde-baeb-2e4956a29494",
   "metadata": {},
   "source": [
    "## generate text from openai api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "777c94e9-9e86-4756-9e99-6b2581e51173",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T23:13:21.562649Z",
     "iopub.status.busy": "2023-05-26T23:13:21.561802Z",
     "iopub.status.idle": "2023-05-26T23:13:21.729133Z",
     "shell.execute_reply": "2023-05-26T23:13:21.727969Z",
     "shell.execute_reply.started": "2023-05-26T23:13:21.562601Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai 0.27.7\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "print(f'openai {openai.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f493227-8fc0-49f9-ad7b-94f6a5dc8ae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T23:13:21.730947Z",
     "iopub.status.busy": "2023-05-26T23:13:21.730358Z",
     "iopub.status.idle": "2023-05-26T23:13:21.809697Z",
     "shell.execute_reply": "2023-05-26T23:13:21.808624Z",
     "shell.execute_reply.started": "2023-05-26T23:13:21.730914Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e172b8c5-b2a4-4de3-b72a-3b8549b7ab50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T13:42:33.629979Z",
     "iopub.status.busy": "2023-06-01T13:42:33.629741Z",
     "iopub.status.idle": "2023-06-01T13:42:33.861200Z",
     "shell.execute_reply": "2023-06-01T13:42:33.860270Z",
     "shell.execute_reply.started": "2023-06-01T13:42:33.629960Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#response = openai.Completion.create(\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#  model=\"text-davinci-003\",\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#  prompt=prompt,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#  temperature=0,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#  max_tokens=20\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mprompt\u001b[49m, response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "#response = openai.Completion.create(\n",
    "#  model=\"text-davinci-003\",\n",
    "#  prompt=prompt,\n",
    "#  temperature=0,\n",
    "#  max_tokens=20\n",
    "#)\n",
    "#print(prompt, response[\"choices\"][0][\"text\"])\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aacbabb-95b4-428a-a059-17776f28c1be",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-26T23:13:22.783669Z",
     "iopub.status.idle": "2023-05-26T23:13:22.784300Z",
     "shell.execute_reply": "2023-05-26T23:13:22.784040Z",
     "shell.execute_reply.started": "2023-05-26T23:13:22.784007Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06a6712f",
   "metadata": {},
   "source": [
    "## generate text from audio/speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93da4828",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T23:42:52.718826Z",
     "start_time": "2023-05-03T23:42:48.241444Z"
    },
    "execution": {
     "iopub.execute_input": "2023-05-26T23:15:46.275130Z",
     "iopub.status.busy": "2023-05-26T23:15:46.274713Z",
     "iopub.status.idle": "2023-05-26T23:15:46.330089Z",
     "shell.execute_reply": "2023-05-26T23:15:46.329044Z",
     "shell.execute_reply.started": "2023-05-26T23:15:46.275091Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2550c828-79d8-4935-a0f9-332afdf9184b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T23:15:47.323941Z",
     "iopub.status.busy": "2023-05-26T23:15:47.323624Z",
     "iopub.status.idle": "2023-05-26T23:15:49.747914Z",
     "shell.execute_reply": "2023-05-26T23:15:49.747080Z",
     "shell.execute_reply.started": "2023-05-26T23:15:47.323902Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}\n"
     ]
    }
   ],
   "source": [
    "pipe = transformers.pipeline(task=\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\",device=device, )\n",
    "print(pipe(prompt, max_new_tokens=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30c9244a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T23:44:13.799368Z",
     "start_time": "2023-05-03T23:43:29.257332Z"
    },
    "execution": {
     "iopub.execute_input": "2023-05-26T23:15:49.749335Z",
     "iopub.status.busy": "2023-05-26T23:15:49.749109Z",
     "iopub.status.idle": "2023-05-26T23:16:18.908843Z",
     "shell.execute_reply": "2023-05-26T23:16:18.907914Z",
     "shell.execute_reply.started": "2023-05-26T23:15:49.749314Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n"
     ]
    }
   ],
   "source": [
    "pipe = transformers.pipeline(task=\"automatic-speech-recognition\",model=\"openai/whisper-large\", device=device)\n",
    "print(pipe(prompt, max_new_tokens=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460ec96-12e7-401e-9e90-656e63069ca6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## cleanup notebook resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3658d-cb10-43da-89ad-f3a225898c90",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-26T23:13:22.796600Z",
     "iopub.status.idle": "2023-05-26T23:13:22.797231Z",
     "shell.execute_reply": "2023-05-26T23:13:22.796942Z",
     "shell.execute_reply.started": "2023-05-26T23:13:22.796913Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "\n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>\n",
    "\n",
    "# code credit: https://github.com/data-science-on-aws/data-science-on-aws/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0810f45-233a-4ede-b663-29a7bbf70b56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
